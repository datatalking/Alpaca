{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data to Strategy with Alpaca\n",
    "Query and process Alpaca OHLCV data. Add features, identify principal components and develop initial trade algorithm.\n",
    "    \n",
    "https://github.com/emskiphoto/Alpaca-Strategy-Development<BR>\n",
    "Copyright 2022 Matt Chmielewski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### References:\n",
    "https://github.com/stefan-jansen/machine-learning-for-trading\n",
    "\n",
    "https://github.com/twopirllc/pandas-ta\n",
    "\n",
    "https://github.com/emskiphoto/alpaca-trade-api-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.5.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (9.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (4.37.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (1.21.6)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Requirement already satisfied: alpaca-trade-api in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.3.0)\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement as (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for as\u001B[0m\u001B[31m\r\n",
      "\u001B[0mRequirement already satisfied: ta in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.10.2)\r\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from ta) (1.21.6)\r\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from ta) (1.3.5)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->ta) (2022.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->ta) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->ta) (1.16.0)\r\n",
      "Requirement already satisfied: pandas_ta in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.3.14b0)\r\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas_ta) (1.3.5)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->pandas_ta) (1.21.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->pandas_ta) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->pandas_ta) (2022.6)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->pandas_ta) (1.16.0)\r\n",
      "Requirement already satisfied: datetime in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.7)\r\n",
      "Requirement already satisfied: zope.interface in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from datetime) (5.5.0)\r\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from datetime) (2022.6)\r\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from zope.interface->datetime) (62.3.2)\r\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (2022.6)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas) (1.21.6)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\r\n",
      "Collecting dotenv\r\n",
      "  Using cached dotenv-0.0.5.tar.gz (2.4 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l-\b \berror\r\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[31m×\u001B[0m \u001B[32mpython setup.py egg_info\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[64 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m   error: subprocess-exited-with-error\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m   × python setup.py egg_info did not run successfully.\r\n",
      "  \u001B[31m   \u001B[0m   │ exit code: 1\r\n",
      "  \u001B[31m   \u001B[0m   ╰─> [16 lines of output]\r\n",
      "  \u001B[31m   \u001B[0m       Traceback (most recent call last):\r\n",
      "  \u001B[31m   \u001B[0m         File \"<string>\", line 36, in <module>\r\n",
      "  \u001B[31m   \u001B[0m         File \"<pip-setuptools-caller>\", line 14, in <module>\r\n",
      "  \u001B[31m   \u001B[0m         File \"/private/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/pip-wheel-hcfhfg9h/distribute_58777639f4284a05baa5266de136a297/setuptools/__init__.py\", line 2, in <module>\r\n",
      "  \u001B[31m   \u001B[0m           from setuptools.extension import Extension, Library\r\n",
      "  \u001B[31m   \u001B[0m         File \"/private/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/pip-wheel-hcfhfg9h/distribute_58777639f4284a05baa5266de136a297/setuptools/extension.py\", line 5, in <module>\r\n",
      "  \u001B[31m   \u001B[0m           from setuptools.dist import _get_unpatched\r\n",
      "  \u001B[31m   \u001B[0m         File \"/private/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/pip-wheel-hcfhfg9h/distribute_58777639f4284a05baa5266de136a297/setuptools/dist.py\", line 7, in <module>\r\n",
      "  \u001B[31m   \u001B[0m           from setuptools.command.install import install\r\n",
      "  \u001B[31m   \u001B[0m         File \"/private/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/pip-wheel-hcfhfg9h/distribute_58777639f4284a05baa5266de136a297/setuptools/command/__init__.py\", line 8, in <module>\r\n",
      "  \u001B[31m   \u001B[0m           from setuptools.command import install_scripts\r\n",
      "  \u001B[31m   \u001B[0m         File \"/private/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/pip-wheel-hcfhfg9h/distribute_58777639f4284a05baa5266de136a297/setuptools/command/install_scripts.py\", line 3, in <module>\r\n",
      "  \u001B[31m   \u001B[0m           from pkg_resources import Distribution, PathMetadata, ensure_directory\r\n",
      "  \u001B[31m   \u001B[0m         File \"/private/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/pip-wheel-hcfhfg9h/distribute_58777639f4284a05baa5266de136a297/pkg_resources.py\", line 1518, in <module>\r\n",
      "  \u001B[31m   \u001B[0m           register_loader_type(importlib_bootstrap.SourceFileLoader, DefaultProvider)\r\n",
      "  \u001B[31m   \u001B[0m       AttributeError: module 'importlib._bootstrap' has no attribute 'SourceFileLoader'\r\n",
      "  \u001B[31m   \u001B[0m       [end of output]\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m   note: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "  \u001B[31m   \u001B[0m error: metadata-generation-failed\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m × Encountered error while generating package metadata.\r\n",
      "  \u001B[31m   \u001B[0m ╰─> See above for output.\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m note: This is an issue with the package mentioned above, not pip.\r\n",
      "  \u001B[31m   \u001B[0m hint: See above for details.\r\n",
      "  \u001B[31m   \u001B[0m /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\r\n",
      "  \u001B[31m   \u001B[0m   SetuptoolsDeprecationWarning,\r\n",
      "  \u001B[31m   \u001B[0m Traceback (most recent call last):\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/setuptools/installer.py\", line 82, in fetch_build_egg\r\n",
      "  \u001B[31m   \u001B[0m     subprocess.check_call(cmd)\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py\", line 347, in check_call\r\n",
      "  \u001B[31m   \u001B[0m     raise CalledProcessError(retcode, cmd)\r\n",
      "  \u001B[31m   \u001B[0m subprocess.CalledProcessError: Command '['/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/tmpk3s7pnvl', '--quiet', 'distribute']' returned non-zero exit status 1.\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m The above exception was the direct cause of the following exception:\r\n",
      "  \u001B[31m   \u001B[0m \r\n",
      "  \u001B[31m   \u001B[0m Traceback (most recent call last):\r\n",
      "  \u001B[31m   \u001B[0m   File \"<string>\", line 36, in <module>\r\n",
      "  \u001B[31m   \u001B[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\r\n",
      "  \u001B[31m   \u001B[0m   File \"/private/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/pip-install-si4gmls6/dotenv_16abe2560a20414799a092b6984ac0e1/setup.py\", line 23, in <module>\r\n",
      "  \u001B[31m   \u001B[0m     scripts=['scripts/dotenv']\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/setuptools/__init__.py\", line 86, in setup\r\n",
      "  \u001B[31m   \u001B[0m     _install_setup_requires(attrs)\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/setuptools/__init__.py\", line 80, in _install_setup_requires\r\n",
      "  \u001B[31m   \u001B[0m     dist.fetch_build_eggs(dist.setup_requires)\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/setuptools/dist.py\", line 879, in fetch_build_eggs\r\n",
      "  \u001B[31m   \u001B[0m     replace_conflicting=True,\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 791, in resolve\r\n",
      "  \u001B[31m   \u001B[0m     replace_conflicting=replace_conflicting\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 1075, in best_match\r\n",
      "  \u001B[31m   \u001B[0m     return self.obtain(req, installer)\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 1087, in obtain\r\n",
      "  \u001B[31m   \u001B[0m     return installer(requirement)\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/setuptools/dist.py\", line 956, in fetch_build_egg\r\n",
      "  \u001B[31m   \u001B[0m     return fetch_build_egg(self, req)\r\n",
      "  \u001B[31m   \u001B[0m   File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/setuptools/installer.py\", line 84, in fetch_build_egg\r\n",
      "  \u001B[31m   \u001B[0m     raise DistutilsError(str(e)) from e\r\n",
      "  \u001B[31m   \u001B[0m distutils.errors.DistutilsError: Command '['/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7', '-m', 'pip', '--disable-pip-version-check', 'wheel', '--no-deps', '-w', '/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/tmpk3s7pnvl', '--quiet', 'distribute']' returned non-zero exit status 1.\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001B[1;31merror\u001B[0m: \u001B[1mmetadata-generation-failed\u001B[0m\r\n",
      "\r\n",
      "\u001B[31m×\u001B[0m Encountered error while generating package metadata.\r\n",
      "\u001B[31m╰─>\u001B[0m See above for output.\r\n",
      "\r\n",
      "\u001B[1;35mnote\u001B[0m: This is an issue with the package mentioned above, not pip.\r\n",
      "\u001B[1;36mhint\u001B[0m: See above for details.\r\n",
      "\u001B[?25hcell 1 done\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install alpaca-trade-api as alpaca_trade_api\n",
    "!pip3 install ta\n",
    "!pip3 install pandas_ta\n",
    "!pip3 install datetime\n",
    "!pip3 install pandas\n",
    "!pip3 install dotenv\n",
    "print(\"cell 1 done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell 2 done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import alpaca_trade_api as tradeapi\n",
    "# from alpaca_trade_api.rest import TimeFrame - Won't work 6-11-21\n",
    "from datetime import datetime, timedelta\n",
    "import timeit\n",
    "import calendar\n",
    "from alpaca_trade_api.rest import REST\n",
    "import pandas_ta as ta\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# from alpaca_trade_api.rest import TimeFrame\n",
    "# Disable jedi autocompleter\n",
    "%config Completer.use_jedi = False\n",
    "print(\"cell 2 done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display and Plot options"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell 3 done\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_rows = 300\n",
    "    # create idx for mutlti-index slicing\n",
    "idx = pd.IndexSlice\n",
    "plt.rcParams['figure.figsize'] = [14,6]\n",
    "print(\"cell 3 done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check connection to Alpaca"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Verify Windows Environment Variables are available\n",
    "API Credentials are stored on Windows PATH variable.  https://dev.to/biplov/handling-passwords-and-secret-keys-using-environment-variables-2ei0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell 4 done\n"
     ]
    }
   ],
   "source": [
    "APCA_API_SECRET_KEY = 'ALr7od44fpwRvDyPktqj7PNPAo0jjy3T1NOz82Iw'\n",
    "APCA_API_KEY_ID = 'PKW9VTG8TDIZ2RJJ80MB'\n",
    "APCA_API_BASE_URL = 'https://data.alpaca.markets/v2'\n",
    "API_credential_vars = ['APCA_API_BASE_URL','APCA_API_KEY_ID', 'APCA_API_SECRET_KEY']\n",
    "# for var in API_credential_vars:\n",
    "#     print(os.getenv(var))\n",
    "print(\"cell 4 done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check Alpaca account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<alpaca_trade_api.rest.REST at 0x7fe579fbe7b8>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APCA_API_SECRET_KEY = 'ALr7od44fpwRvDyPktqj7PNPAo0jjy3T1NOz82Iw'\n",
    "APCA_API_KEY_ID = 'PKW9VTG8TDIZ2RJJ80MB'\n",
    "APCA_API_BASE_URL = 'https://data.alpaca.markets/v2'\n",
    "\n",
    "api = tradeapi.REST()\n",
    "# Get our account information.\n",
    "account = api.get_account()\n",
    "# Check if our account is restricted from trading.\n",
    "api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$200000 is available as buying power.\n",
      "cell 5 done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if account.trading_blocked:\n",
    "    print('Account is currently restricted from trading.')\n",
    "# Check how much money we can use to open new positions.\n",
    "print('${} is available as buying power.'.format(account.buying_power))\n",
    "print(\"cell 5 done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Download Price History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get list of assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                     id      class exchange symbol  \\\n0  8444480d-d13a-4fa9-adde-6225431d1753  us_equity      OTC  REVRQ   \n1  4e872f2e-f5ad-417b-9f12-832fb16c03cb  us_equity      OTC  BRCNF   \n\n                        name  status  tradable  marginable  \\\n0    REVLON INC Common Stock  active     False       False   \n1  Burcon NutraScience Corp.  active     False       False   \n\n   maintenance_margin_requirement  shortable  easy_to_borrow  fractionable  \\\n0                             100      False           False         False   \n1                             100      False           False         False   \n\n  min_order_size min_trade_increment price_increment  \n0            NaN                 NaN             NaN  \n1            NaN                 NaN             NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>class</th>\n      <th>exchange</th>\n      <th>symbol</th>\n      <th>name</th>\n      <th>status</th>\n      <th>tradable</th>\n      <th>marginable</th>\n      <th>maintenance_margin_requirement</th>\n      <th>shortable</th>\n      <th>easy_to_borrow</th>\n      <th>fractionable</th>\n      <th>min_order_size</th>\n      <th>min_trade_increment</th>\n      <th>price_increment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8444480d-d13a-4fa9-adde-6225431d1753</td>\n      <td>us_equity</td>\n      <td>OTC</td>\n      <td>REVRQ</td>\n      <td>REVLON INC Common Stock</td>\n      <td>active</td>\n      <td>False</td>\n      <td>False</td>\n      <td>100</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4e872f2e-f5ad-417b-9f12-832fb16c03cb</td>\n      <td>us_equity</td>\n      <td>OTC</td>\n      <td>BRCNF</td>\n      <td>Burcon NutraScience Corp.</td>\n      <td>active</td>\n      <td>False</td>\n      <td>False</td>\n      <td>100</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_asset_df_alpaca(api, status='active'):\n",
    "    \"\"\"Returns a list of assets and trading characteristics as a dataframe\"\"\"\n",
    "    assets = api.list_assets(status=status)\n",
    "    return pd.DataFrame.from_records([asset._raw for asset in assets.__iter__()])\n",
    "df_assets = build_asset_df_alpaca(api)\n",
    "df_assets.head(2)\n",
    "# print(\"cell 6 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                       id  class\nexchange marginable             \nAMEX     True         280    280\nARCA     True        1936   1936\nBATS     True         575    575\nFTXU     False         52     52\nNASDAQ   True        4995   4995\nNYSE     True        2871   2871\nOTC      False        375    375",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>id</th>\n      <th>class</th>\n    </tr>\n    <tr>\n      <th>exchange</th>\n      <th>marginable</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AMEX</th>\n      <th>True</th>\n      <td>280</td>\n      <td>280</td>\n    </tr>\n    <tr>\n      <th>ARCA</th>\n      <th>True</th>\n      <td>1936</td>\n      <td>1936</td>\n    </tr>\n    <tr>\n      <th>BATS</th>\n      <th>True</th>\n      <td>575</td>\n      <td>575</td>\n    </tr>\n    <tr>\n      <th>FTXU</th>\n      <th>False</th>\n      <td>52</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>NASDAQ</th>\n      <th>True</th>\n      <td>4995</td>\n      <td>4995</td>\n    </tr>\n    <tr>\n      <th>NYSE</th>\n      <th>True</th>\n      <td>2871</td>\n      <td>2871</td>\n    </tr>\n    <tr>\n      <th>OTC</th>\n      <th>False</th>\n      <td>375</td>\n      <td>375</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assets.groupby(['exchange','marginable']).count().iloc[:,:2]\n",
    "# print(\"cell 7 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "us_equity    11032\ncrypto          52\nName: class, dtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assets['class'].value_counts()\n",
    "# print(\"cell 8 done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build OHLCV Database\n",
    "https://alpaca.markets/docs/api-documentation/api-v2/market-data/alpaca-data-api-v2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set list of Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11084 assets total\n",
      "There are 100 assets in assets_query\n"
     ]
    },
    {
     "data": {
      "text/plain": "['ACHHY',\n 'ACMSY',\n 'ACUT',\n 'ADXS',\n 'AFIIQ',\n 'ALFIQ',\n 'ALJJ',\n 'AMPE',\n 'ARGGY',\n 'AXAS']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {df_assets[\"symbol\"].nunique()} assets total')\n",
    "# assets_query = df_assets['symbol'].sample(10).sort_values().to_list()\n",
    "assets_query = df_assets['symbol'][:100].sort_values().to_list()\n",
    "print(f'There are {len(assets_query)} assets in assets_query')\n",
    "assets_query[:10]\n",
    "# print(\"cell 9 done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The rate limit is 200 requests every minute per API key.<BR>\n",
    "All date time type inputs and outputs are serialized according to ISO8601<BR>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell 10 done\n"
     ]
    }
   ],
   "source": [
    "# custom function\n",
    "def build_multi_symbol_df(assets,start_date = '2021-01-01', end_date = '2021-05-25',\n",
    "                            frequency='15Min', TZ = 'America/New_York', limit=1000):\n",
    "    \"\"\"Returns single multi-index OHLCV time series dataframe for list of assets over input time\n",
    "    range using Alpaca's API 'get_bars()' method.  'assets' must be input as list.\n",
    "    Output multi-index is:  symbol, timestamp\n",
    "    and output columns are: open, high, low, close, volume.\n",
    "   \n",
    "    frequency options:\n",
    "    Day = \"1Day\"\n",
    "    Hour = \"1Hour\"\n",
    "    15 Minute = '15Min'\n",
    "    Minute = \"1Min\"\n",
    "    Sec = \"1Sec\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # reference problematic query (missing bars - not used here):\n",
    "    # TZ = 'America/New_York'\n",
    "    # bars = api.get_bars('AAQC.U', '5Min',\n",
    "                         # end=pd.Timestamp('2021-05-19', tz=TZ).isoformat(),\n",
    "                           # limit=1000).df.droplevel(0,axis=1)\n",
    "    # Sort assets for the benefit of future indexing\n",
    "    assets = sorted(assets)\n",
    "    # Get Name of columns for temp df\n",
    "    n_assets = len(assets)\n",
    "    print(f'{n_assets} total assets to query')\n",
    "    start_date_plus_1 = datetime.strftime(datetime.strptime(start_date,\"%Y-%m-%d\") + timedelta(days=10),\"%Y-%m-%d\" )\n",
    "\n",
    "    # Build empty DataFrame\n",
    "    # Get names of columns based on short query of first asset\n",
    "    columns = list(api.get_bars(assets[0], frequency,\n",
    "        start=start_date, end=start_date_plus_1).df.droplevel(0,axis=1).columns)\n",
    "\n",
    "    # Create empty dataframe as multiindex to hold bars responses\n",
    "    temp = pd.DataFrame(columns=['symbol',\n",
    "             'timestamp'] + columns).set_index(['symbol','timestamp']).asfreq(frequency)\n",
    "    bars_ = []\n",
    "    bars = []\n",
    "\n",
    "    # Set dates\n",
    "    end_date_range = pd.to_datetime(pd.Timestamp(end_date, tz=TZ).isoformat())\n",
    "    start_date_dt = pd.to_datetime(pd.Timestamp(start_date, tz=TZ).isoformat())\n",
    "    print(start_date_dt, \"\\n\", end_date_range)\n",
    "\n",
    "    # Iterate through assets:\n",
    "    for idx, symbol in enumerate(assets):\n",
    "        # update start_date_dt\n",
    "        print(f'== {idx + 1} of {n_assets} querying \"{symbol}\" from {start_date_dt} to {end_date_range} ==')\n",
    "\n",
    "        # limit date range to not exceed API's bars limit\n",
    "        count = 0\n",
    "        end_date_dt = end_date_range\n",
    "        last_index_start = end_date_range\n",
    "        while end_date_dt.to_pydatetime() >= start_date_dt.to_pydatetime():\n",
    "            count+=1\n",
    "            # print(f'end Date is: {end_date_dt}')\n",
    "            print(f'Chunk {count} - {symbol} desired end date:  {end_date_dt}')\n",
    "\n",
    "            # Chunked query starts with end date and works backwards in time to start date\n",
    "            bars = api.get_bars(symbol, frequency,\n",
    "                                  end=end_date_dt.isoformat(),\n",
    "                                  limit=limit).df.droplevel(0,axis=1)\n",
    "\n",
    "            print(f'Bars shape:  {bars.shape}')\n",
    "            # Check if bars is empty, if so break out of loop\n",
    "            try:\n",
    "                bars.index[0]\n",
    "            except:\n",
    "                break\n",
    "            # Break out of loop if start date repeats (indicating no history prior to start date available)\n",
    "            if bars.index[0].to_pydatetime() == last_index_start:\n",
    "                print(f'<<<<<  No history prior to {last_index_start} available for {symbol}  >>>>>\\n\\n')\n",
    "                break\n",
    "            last_index_start = bars.index[0].to_pydatetime()\n",
    "            print(f'{symbol} chunk {count} index start:  {bars.index[0]}')\n",
    "            # print(f'Start of barset index {bars.index[0]}')\n",
    "            print(f'{symbol} chunk {count} index end:   {bars.index[-1]}')\n",
    "            # If an empty 'bars' is returned, skip to next asset\n",
    "            if len(bars.index) < 1:\n",
    "                break\n",
    "            \n",
    "            # end of date range based on chunk size:\n",
    "            # For testing:\n",
    "            # start_date_dt = datetime.strptime(start_date_str,\"%Y-%m-%d\") + timedelta(days=10)\n",
    "\n",
    "            # Reset chunk end date to be one period before the start of the last chunk that was queried\n",
    "            end_date_dt = pd.Timestamp(bars.index[0]) - pd.Timedelta(frequency)\n",
    "            print(f'Is {end_date_dt.to_pydatetime()} less than {start_date_dt.to_pydatetime()}'\n",
    "                  f': {end_date_dt.to_pydatetime() < start_date_dt.to_pydatetime()}\\n')\n",
    "            # slice off records with dates outside of input start and end dates and set index frequency of df\n",
    "            # bars = bars.loc[start_date:end_date]\n",
    "            # bars = bars.loc[start_date:end_date]\n",
    "            bars = bars.loc[start_date:end_date].asfreq(frequency).sort_index()\n",
    "            # print(bars.index)\n",
    "            # Create multi-index as list of tuples\n",
    "            unique_timestamps = bars.index.unique()\n",
    "            idx_tuples = [(symbol,timestamp) for timestamp in unique_timestamps]\n",
    "            # idx_tuples = [(symbol,timestamp) for timestamp in bars.index]\n",
    "            bars.set_index(pd.Index(idx_tuples), inplace=True)\n",
    "            bars.rename_axis(['symbol','timestamp'], inplace=True)\n",
    "            bars_.append(bars)\n",
    "        \n",
    "            # Drop duplicate indices, if needed\n",
    "            # unique_idx = bars_.index.drop_duplicates(keep='first')\n",
    "            # bars_ = bars.loc[unique_idx]\n",
    "        \n",
    "    # completed compiling bars_, now commit to dataframe\n",
    "    df = pd.concat([temp] + bars_)\n",
    "\n",
    "    # set frequency of datetime index\n",
    "    df.index.levels[1].freq = frequency\n",
    "\n",
    "    # If there is a volume column fill any NaN and convert to integer\n",
    "    if 'volume' in df.columns:\n",
    "        # df['volume'] = df['volume'].interpolate().astype('int')\n",
    "        df['volume'] = df['volume'].astype('int', errors='ignore')\n",
    "        # df.index = df.index.tz_convert('America/New_York')\n",
    "        # Sort symbols and timestamps alphabetically and chronologically\n",
    "\n",
    "        # df.sort_index(level=['symbol','timestamp'], inplace=True)\n",
    "\n",
    "    # remove records where OHLC are all NaN\n",
    "    df = df.dropna(subset=['open','high','low','close'], how='all')\n",
    "\n",
    "    return df\n",
    "    del bars_, df\n",
    "print(\"cell 10 done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Query assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Asset query parameters\n",
    "In the future, implement a 'class' to manage attributes of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell 11 done\n"
     ]
    }
   ],
   "source": [
    "frequency = '15Min'\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2021-05-30'\n",
    "TZ = 'America/New_York'\n",
    "OHLC_cols = ['open','high','low','close']\n",
    "print(\"cell 11 done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 total assets to query\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot remove 1 levels from an index with 1 levels: at least one level must be left.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/ipykernel_90521/528272910.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;31m# df = build_multi_symbol_df(assets_query[:10],start_date = '2020-01-01', end_date = '2021-05-25',\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m                 \u001B[0;31m# frequency='5Min', TZ = 'America/New_York', limit=1000)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_multi_symbol_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_date\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstart_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_date\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mend_date\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrequency\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfrequency\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTZ\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTZ\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlimit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cell 12 done\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/v0/7gtj3bhx74z6w7kx78fhqlmm0000gn/T/ipykernel_90521/1937682697.py\u001B[0m in \u001B[0;36mbuild_multi_symbol_df\u001B[0;34m(assets, start_date, end_date, frequency, TZ, limit)\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0;31m# Get names of columns based on short query of first asset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m     columns = list(api.get_bars(assets[0], frequency,\n\u001B[0;32m---> 32\u001B[0;31m         start=start_date, end=start_date_plus_1).df.droplevel(0,axis=1).columns)\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0;31m# Create empty dataframe as multiindex to hold bars responses\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mdroplevel\u001B[0;34m(self, level, axis)\u001B[0m\n\u001B[1;32m    864\u001B[0m         \"\"\"\n\u001B[1;32m    865\u001B[0m         \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_axis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 866\u001B[0;31m         \u001B[0mnew_labels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdroplevel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    867\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_axis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_labels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mdroplevel\u001B[0;34m(self, level)\u001B[0m\n\u001B[1;32m   1860\u001B[0m         \u001B[0mlevnums\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_level_number\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlev\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlev\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1861\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1862\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_drop_level_numbers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlevnums\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1863\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1864\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mfinal\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36m_drop_level_numbers\u001B[0;34m(self, levnums)\u001B[0m\n\u001B[1;32m   1872\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlevnums\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlevels\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1873\u001B[0m             raise ValueError(\n\u001B[0;32m-> 1874\u001B[0;31m                 \u001B[0;34mf\"Cannot remove {len(levnums)} levels from an index with \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1875\u001B[0m                 \u001B[0;34mf\"{self.nlevels} levels: at least one level must be left.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1876\u001B[0m             )\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot remove 1 levels from an index with 1 levels: at least one level must be left."
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "\n",
    "# list of particularly messy symbols:\n",
    "test_list = ['ACSI', 'AHT', 'TMAT', 'AROC', 'BBUS', 'CAT', 'AMAM']\n",
    "\n",
    "# df = build_multi_symbol_df(assets_query[:10],start_date = '2020-01-01', end_date = '2021-05-25',\n",
    "                # frequency='5Min', TZ = 'America/New_York', limit=1000)\n",
    "df = build_multi_symbol_df(test_list, start_date = start_date, end_date = end_date, frequency=frequency, TZ = TZ, limit=1000)\n",
    "print(\"cell 12 done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save df to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_csv(f'data/{df.index.levels[0].nunique()}_symbols_{start_date}_to_{end_date}_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Was full time range of data queried?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_date == datetime.strftime(df.index.levels[1].min(), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "end_date == datetime.strftime(df.index.levels[1].max(), \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_date, df.index.levels[1].min(), end_date, df.index.levels[1].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Are there gaps in the dates or times of the time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "symbols_by_close_count = df.groupby(level=0).count().sort_values(by='close').index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[idx[symbols_by_close_count[0],:]].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Are all index values spaced evenly?\n",
    "for symbol in symbols_by_close_count:\n",
    "    uniform_spacing = all(np.diff(df.loc[idx[symbols_by_close_count[-1],:]].index)==\n",
    "        np.diff(df.loc[idx[symbols_by_close_count[-1],:]].index)[0])\n",
    "    print(f'{symbol}:  {uniform_spacing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[idx[symbols_by_close_count[1],:]].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### How many rows of data does each symbol have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(level=0).count().sort_values(by='close')['close'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Clean & Filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Infer market days and market hours from df\n",
    "Data supplied contains market and after hours data. Identifying market days and market hours enables segregation of market and non-market hours data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def aggregate_daily_trade_count_from_df(df, n_symbols=5, min_trade_count_pct = 0.025):\n",
    "    \"\"\"Returns numerical indices for only df records containing dates with measurable market activity\n",
    "    (df record count > min_trade_count_pct).  \n",
    "    n_symbols: the minimum number of symbols in the df\n",
    "    that are considered out of all symbols in df.\n",
    "    min_trade_count_pct:  the minimum normalized count percentage of historical trades (i.e. total\n",
    "    aggregated trade count for a given hour divided my max trade count of any hour) needed\n",
    "    to distinguish low volume trading hours (i.e. outside market hours) from high volume\n",
    "    trading hours\"\"\"\n",
    "#     create idx for mutlti-index slicing\n",
    "    idx = pd.IndexSlice\n",
    "#     find symbols with highest rates of complete data\n",
    "    symbols = df.index.get_level_values(0).unique().to_list()\n",
    "    counts = [df.loc[idx[symbol,:],'close'].count() for symbol in symbols]\n",
    "    lengths = [df.loc[idx[symbol,:],'close'].shape[0] for symbol in symbols]\n",
    "    pct_completes = [count/length for count, length in zip(counts, lengths)]\n",
    "#     obtain indices of largest pct_completes values, then analyze top n symbols \n",
    "    pct_ranked_idx = sorted(range(len(pct_completes)), key=lambda x: pct_completes[x],reverse=True)\n",
    "    analysis_symbols = [symbols[x] for x in pct_ranked_idx][:min(n_symbols,len(symbols))]\n",
    "#     Get aggregate count of transactions by hour of day then normalize values\n",
    "    agg_trade_count_by_day = df.loc[idx[analysis_symbols,:],'close']\\\n",
    "        .groupby(df.loc[idx[analysis_symbols,:],'close'].index.get_level_values(1).date).count()\n",
    "    return agg_trade_count_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "agg_trade_count_by_day = aggregate_daily_trade_count_from_df(df, n_symbols=5, min_trade_count_pct = 0.025)\n",
    "agg_trade_count_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# agg_trade_count_by_day.plot(kind='bar',figsize=(12,5))\n",
    "agg_trade_count_by_day.plot(kind='hist', bins=50,figsize=(12,3), grid='both')\n",
    "plt.xlabel('Daily Trade Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def market_days_from_df(agg_trade_count_by_day):\n",
    "    \"\"\"Function assumes any dates missing in input df are days where the \n",
    "    market is closed.  Returns datetime series of market days\"\"\"\n",
    "    return pd.to_datetime(agg_trade_count_by_day.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "market_dates = market_days_from_df(agg_trade_count_by_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "market_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Determine Market Hours from source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def infer_market_hours_from_df(df, n_symbols=5, min_trade_count_pct = 0.2):\n",
    "    \"\"\"Returns 'market_open' and 'market_close' from multi-index df containing full-day \n",
    "    time-series data.  \n",
    "    n_symbols: the minimum number of symbols in the df\n",
    "    that are considered out of all symbols in df.\n",
    "    min_trade_count_pct:  the minimum normalized count percentage of historical trades (i.e. total\n",
    "    aggregated trade count for a given hour divided my max trade count of any hour) needed\n",
    "    to distinguish low volume trading hours (i.e. outside market hours) from high volume\n",
    "    trading hours\"\"\"\n",
    "#     create idx for mutlti-index slicing\n",
    "    idx = pd.IndexSlice\n",
    "#     find symbols with highest rates of complete data\n",
    "    symbols = df.index.get_level_values(0).unique().to_list()\n",
    "    counts = [df.loc[idx[symbol,:],'close'].count() for symbol in symbols]\n",
    "    lengths = [df.loc[idx[symbol,:],'close'].shape[0] for symbol in symbols]\n",
    "    pct_completes = [count/length for count, length in zip(counts, lengths)]\n",
    "#     obtain indices of largest pct_completes values, then analyze top n symbols \n",
    "    pct_ranked_idx = sorted(range(len(pct_completes)), key=lambda x: pct_completes[x],reverse=True)\n",
    "    analysis_symbols = [symbols[x] for x in pct_ranked_idx][:min(n_symbols,len(symbols))]\n",
    "#     Get aggregate count of transactions by hour of day then normalize values\n",
    "    agg_trade_count_by_time = df.loc[idx[analysis_symbols,:],'close']\\\n",
    "        .groupby(df.loc[idx[analysis_symbols,:],'close'].index.get_level_values(1).time).count()\n",
    "    max_count = agg_trade_count_by_time.max()\n",
    "    normed_agg_trade_count_by_time = agg_trade_count_by_time/max_count\n",
    "    # Identify first and last hour where aggregate transaction count > min_trade_count_pct\n",
    "    times_w_count_gt_threshold = [idx for idx in normed_agg_trade_count_by_time.index if normed_agg_trade_count_by_time[idx] > min_trade_count_pct  ]\n",
    "    market_open, market_close = times_w_count_gt_threshold[0],times_w_count_gt_threshold[-1]\n",
    "    return market_open, market_close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "market_open, market_close = infer_market_hours_from_df(df, n_symbols=10)\n",
    "market_open, market_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create a perfect, complete datetime index to reindex raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datetime_idx_complete = pd.date_range(start=df.index.levels[1].min(), end=df.index.levels[1].max(), freq=df.index.levels[1].freq)\n",
    "datetime_idx_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Are there duplicate timestamps within a given symbol's index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_for_duplicated_dt_indices(df):\n",
    "    print('Does symbol have duplicated timestamps in datetime index?')\n",
    "    symbol_dt_index_duplicated = []\n",
    "    duplicated_dt_index = {}\n",
    "    symbols = df.index.get_level_values(0).unique()\n",
    "    for symbol in symbols:\n",
    "        duplicated_dt_index = df.loc[idx[symbol,:]].index.has_duplicates\n",
    "        print(f'{symbol}: {duplicated_dt_index}')    \n",
    "        symbol_dt_index_duplicated.append(duplicated_dt_index)\n",
    "    duplicated_dt_index = dict(zip(symbols,symbol_dt_index_duplicated ))\n",
    "    return duplicated_dt_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "duplicated_dt_index = check_for_duplicated_dt_indices(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fill gaps in datetime index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# just in case....\n",
    "df_backup = df.copy()\n",
    "# df = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fill_OHLCV_gaps(df):\n",
    "    \"\"\"Returns complete time series from input OHLCV time series dataframe.\n",
    "    Input dataframe must have timezone aware datetime index with a defined \n",
    "    frequency and can be single or dual indexed (ex. ['symbol', 'timestamp']).   \n",
    "    NaN records leading up to first non-NaN record are returned unaltered.\n",
    "    \n",
    "    The OHLC data \n",
    "    is forward filled and NaNs in the volume data should be filled\n",
    "    with '0'. This is because NaNs in price data should carry forward the\n",
    "    last prices - the assumption is that if there are no new transactions \n",
    "    (i.e. volume = 0) then the most recent valid price is also the current\n",
    "    valid price. Volume NaNs should be filled with '0' because if there were\n",
    "    no prices or volume reported for a given timestamp it means there was no volume.\n",
    "    Goal is to revise time series so that no records are NaN in the time series and\n",
    "    no timestamp gaps in the time series sequence.\"\"\"\n",
    "    \n",
    "    OHLC_cols = ['open','high','low','close']\n",
    "    \n",
    "    def process_df(df, symbol, datetime_idx_complete):\n",
    "        \"\"\"Different fill logic is needed for OHLC and volume data. \n",
    "        The OHLC data should be forward filled and NaNs in the volume\n",
    "        data should be filled with '0'. This is because NaNs in price\n",
    "        data should carry forward the last prices - the assumption is\n",
    "        that if there are no new transactions (volume = 0) than the most \n",
    "        recent valid price is also the current valid price. Volume NaNs \n",
    "        should be filled with '0' because if there were no prices or volume\n",
    "        reported for a given timestamp it means there was no volume.\n",
    "        \n",
    "        Leading NaN OHLC records with 0 volume remain NaN - that is OK as \n",
    "        we don't want to back fill leading NaNs.  It's better to not have \n",
    "        the data and omit the first n records than to force assumed values \n",
    "        on leading data.\n",
    "\n",
    "        It can be seen that OHLC prices for 0-volume intervals are not NaN \n",
    "        and remain constant over intervals until volume > 0 - this is good.  \"\"\"\n",
    "    #             create df single index df and reindex to achieve complete datetime index\n",
    "        df = df.reindex(datetime_idx_complete)\n",
    "#                 fill NaN in volume columns with 0\n",
    "\n",
    "        df['volume'] = df['volume'].fillna(0) \n",
    "#                 forward fill OHLC columns\n",
    "        df[OHLC_cols] = df[OHLC_cols].ffill()\n",
    "        return df\n",
    "        \n",
    "#     check if df is a multi-index\n",
    "    is_multi_idx = isinstance(df.index, pd.MultiIndex)\n",
    "    \n",
    "    if is_multi_idx:\n",
    "        idx = pd.IndexSlice\n",
    "    #     generate a complete datetime index for dates in between min and max dates\n",
    "#     this will be the new datetime index\n",
    "        datetime_idx_complete = pd.date_range(start=df.index.levels[1].min(),\n",
    "                                          end=df.index.levels[1].max(),\n",
    "                                          freq=df.index.levels[1].freq)\n",
    "        symbols = df.index.levels[0].sort_values().unique()\n",
    "#         dictionary to store multiple dfs\n",
    "        symbols_dict = {}\n",
    "        for symbol in symbols:\n",
    "            symbols_dict[symbol] = process_df(df.loc[idx[symbol,:]], symbol, datetime_idx_complete)\n",
    "# combine all dfs stored in symbols_dict as multi-index\n",
    "        df = pd.concat(symbols_dict)\n",
    "            \n",
    "    else:\n",
    "        print('Function Not Ready Yet')\n",
    "        datetime_idx_complete = pd.date_range(start=df.index[0],\n",
    "                                          end=df.index[-1],\n",
    "                                          freq=df.index.freq)\n",
    "        df = process_df(df, symbol, datetime_idx_complete)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = fill_OHLCV_gaps(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for symbol in df.index.levels[0].unique():\n",
    "    df.loc[idx[symbol], 'close'].plot(figsize=(12,4), grid='both')\n",
    "    plt.ylabel('close ($)')\n",
    "    plt.title(f'{symbol} - {start_date} to {end_date}')\n",
    "    print(symbol)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### How many rows of a reindexed time series are NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df.loc[idx['ACSI',:]].reindex(datetime_idx_complete).isna().sum(axis=1).value_counts()\n",
    "df.loc[idx[symbols_by_close_count[0],:]].reindex(datetime_idx_complete).isna().sum(axis=1).value_counts().head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Slice df to market hours only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def slice_to_market_hours(df, market_open, market_close):\n",
    "    from_ = market_open.strftime(\"%H:%M\")\n",
    "    to_ = market_close.strftime(\"%H:%M\")\n",
    "    idxs = df.index.get_level_values(1).indexer_between_time(from_, to_)\n",
    "    return df.iloc[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(df.shape)\n",
    "df_mkt_hours = slice_to_market_hours(df, market_open, market_close)\n",
    "df_mkt_hours.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(df_mkt_hours.index.get_level_values(1).hour).agg([min, max]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = df.index.get_level_values(1).hour.value_counts().sort_index().plot(kind='bar', figsize=(12,4), grid='y', label='Timestamp Count')\n",
    "x_min_mkt, x_max_mkt = pd.Series(df_mkt_hours.index.get_level_values(1).hour).agg([min, max]).values\n",
    "# plt.fill_betweenx(1, )\n",
    "plt.axvspan(x_min_mkt*0.98, x_max_mkt*1.02, color='green', alpha=0.2, label='Market Hours')\n",
    "plt.xlabel('hour')\n",
    "plt.title(f'Aggregate count of index timestamps by hour for {df.index.get_level_values(0).nunique()} symbols')\n",
    "y_min, y_max = df.index.get_level_values(1).hour.value_counts().agg([min,max]).values * [0.99,1.01]\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt_hours.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for symbol in df_mkt_hours.index.levels[0].unique():\n",
    "    df_mkt_hours.loc[idx[symbol], 'close'].plot(figsize=(12,4))\n",
    "    plt.ylabel('close')\n",
    "    plt.title(f'{symbol}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Slice df to market days only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def infer_market_days(df, n_symbols=10):\n",
    "    \"\"\"Distinguish market days from non-market (market closed days) based on daily volumes.\n",
    "    Daily mean volumes are divided in to a number of bins that is equal to the number of weeks\n",
    "    in the data.  The upper edge of the lowest volume bin is used to define the threshold between \n",
    "    active market volume and a closed market.\n",
    "\n",
    "    Function is intended for multi-index dfs with data from the same market.  Do not use\n",
    "    for a mix of markets.   The more symbols with more volume in the df, the more reliable\n",
    "    the result.  It is assumed that all symbols in df are in the same market/exchange and\n",
    "    all follow the same 'market open' dates and times\"\"\"\n",
    "    from numpy import histogram\n",
    "    idx = pd.IndexSlice\n",
    "    #     Slice df to a subset of n_symbols.  Sort symbols by count of non-NaN records and select\n",
    "    # the top n_symbols by valid record count\n",
    "    n_symbols = min(n_symbols,len(df.index.get_level_values(0).unique()))\n",
    "    top_n_symbols = df.groupby(level=0)['open'].count()\\\n",
    "                    .sort_values(ascending=False).index.tolist()[:n_symbols]\n",
    "    #     want to identify days where market is closed, (including holidays) so resample to days\n",
    "    df_daily_volume = df.loc[idx[top_n_symbols,:], 'volume'].resample('D', level=1).mean()\n",
    "    n_weeks = df_daily_volume.shape[0]//7\n",
    "    hist_ = histogram(df_daily_volume, bins=n_weeks)\n",
    "    #     This is the right hand edge of the first histogram bin\n",
    "    volume_threshold = hist_[1][1]\n",
    "    # identify days with volume > volume_threshold as market_open_days\n",
    "    volume_GT_threshold = df_daily_volume > volume_threshold\n",
    "    market_open_days = df_daily_volume.index.date[volume_GT_threshold]\n",
    "    # set up filter to reduce df to only the days in market_open_days\n",
    "    # which dates are market_open_dates?\n",
    "    market_open_bool = pd.Series(df.index.get_level_values(1).date).isin(market_open_days)\n",
    "#     OPTIONAL:  compare output df dates to market_open_days:\n",
    "# pd.Series(sorted(list(set(df_mkt.index.get_level_values(1).floor('D')\\\n",
    "# .date))) == market_open_days).value_counts()\n",
    "    return df.loc[idx[market_open_bool.values,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "display(df_mkt_hours.shape)\n",
    "df_mkt = infer_market_days(df_mkt_hours)\n",
    "df_mkt.index.names = ['symbol', 'timestamp']\n",
    "df_mkt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.loc[df_mkt['volume']>0, 'volume'].hist(bins=73)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Trade Volume')\n",
    "plt.title(f'Distribution of Volume by tick for non-zero '\n",
    "          f'volume ticks for all {df_mkt.index.levels[0].nunique()} symbols')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for symbol in df_mkt.index.levels[0].unique():\n",
    "    df_mkt.loc[idx[symbol], 'close'].plot(figsize=(12,4))\n",
    "    plt.ylabel('close')\n",
    "    plt.title(f'{symbol}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### How Many NaNs remain?\n",
    "At this point it is expected that only blocks of leading NaNs could be present.  Drop these and effectively remove the beginning of time series history for specific stocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.loc[df_mkt.isna().any(1)][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.isna().sum(1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.dropna(inplace=True, subset=OHLC_cols)\n",
    "df_mkt.loc[df_mkt.isna().any(1)][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Filter Noise in Data\n",
    "Remember that noise filtering steps will also need to be applied to incoming data in live strategies.  For this reason, the approach here is to correct only blatant extreme values.  Otherwise noise will be left in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for symbol in df_mkt.index.levels[0].unique():\n",
    "#     df[OHLC_cols].replace(0, np.nan).bfill().where(df_mkt.ffill().notnull())\n",
    "    df_mkt.loc[idx[symbol], 'close'].plot(figsize=(12,4), color='blue', label='raw zeros')\n",
    "    df_mkt.loc[idx[symbol], 'close'].replace(0, np.nan).bfill().where(df_mkt.ffill().notnull()).plot(figsize=(12,4), color='red', label='fill zeros')\n",
    "    plt.title(f'{symbol}')\n",
    "    plt.xlabel('close')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Compare methods of filling 0's - Interpolate seems best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CAT_close_2021_03 = df_mkt.loc[idx['CAT',\"2021-03-02\":\"2021-03-04\"], 'close']\n",
    "CAT_close_2021_03.plot(label='raw zeros')\n",
    "CAT_close_2021_03.replace(0, np.nan).bfill().where(CAT_close_2021_03.ffill()\\\n",
    "           .notnull()).plot(figsize=(12,4), color='red', label='filled zeros').plot()\n",
    "CAT_close_2021_03.replace(0,np.nan).interpolate(limit_area='inside')\\\n",
    "            .plot(label='interpolated', color='green', marker='x')\n",
    "# CAT_close_2021_03.replace(0, np.nan).groupby('timestamp').transform(pd.DataFrame.interpolate)\n",
    "# plt.xticks(ticks=np.arange(0,CAT_close_2021_03.shape[0]), labels = CAT_close_2021_03.index.levels[1], rotation = 75)\n",
    "plt.xticks(rotation = 75)\n",
    "plt.ylim(CAT_close_2021_03.quantile(0.5)*0.95, CAT_close_2021_03.quantile(0.5)*1.05)\n",
    "plt.legend()\n",
    "plt.grid('both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fill zeros in OHLC of df_mkt and save as df_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(df_mkt[OHLC_cols] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfs_no_zero = {}\n",
    "symbols = df_mkt.index.levels[0].unique()\n",
    "# slice simple index df's out of multi-index df\n",
    "for symbol in symbols:\n",
    "    dfs_no_zero[symbol] = df_mkt.loc[idx[symbol], OHLC_cols].replace(0,np.nan).interpolate(limit_area='inside')\n",
    "#     dfs_no_zero[symbol] = df_mkt.loc[idx[symbol], OHLC_cols].replacbe(0,np.nan).interpolate(method='index', limit_area='inside')\n",
    "#     print(symbol)\n",
    "df_denoised = pd.concat(dfs_no_zero,keys=symbols).assign(volume = df_mkt['volume'])\n",
    "del dfs_no_zero\n",
    "df_denoised['volume'] = df_mkt['volume'].copy()\n",
    "# df_denoised.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(df_denoised[OHLC_cols] == 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Compare Raw OHLC data to denoised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for symbol in df_mkt.index.levels[0].unique():\n",
    "    df_mkt.loc[idx[symbol], 'close'].plot(figsize=(12,4), color='blue', label='raw')\n",
    "#     df_mkt.loc[idx[symbol], 'close'].replace(0, np.nan).groupby('timestamp')\\\n",
    "#         .transform(pd.DataFrame.interpolate, limit_area='inside')\\\n",
    "#         .plot(figsize=(12,4), color='orange', linestyle = '--')\n",
    "    df_denoised.loc[idx[symbol], 'close'].plot(figsize=(12,4), color='black', lw=1.5, linestyle='--', label='denoised')\n",
    "    plt.legend()\n",
    "    print(symbol)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_mkt.loc[idx['TMAT'], 'close'].replace(0, np.nan).groupby('timestamp')\\\n",
    "        .transform(pd.DataFrame.interpolate, limit_area='inside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CAT_close_2021_03 = df_mkt.loc[idx['CAT',\"2021-03-02\":\"2021-03-04\"], 'close']\n",
    "CAT_close_2021_03_dn = df_denoised.loc[idx['CAT',\"2021-03-02\":\"2021-03-04\"], 'close']\n",
    "CAT_close_2021_03.plot(label='raw zeros')\n",
    "# CAT_close_2021_03.replace(0, np.nan).bfill().where(CAT_close_2021_03.ffill()\\\n",
    "#            .notnull()).plot(figsize=(12,4), color='red', label='filled zeros').plot()\n",
    "CAT_close_2021_03_dn\\\n",
    "            .plot(label='denoised', color='green', marker='x')\n",
    "# CAT_close_2021_03.replace(0, np.nan).groupby('timestamp').transform(pd.DataFrame.interpolate)\n",
    "plt.xticks(rotation = 75)\n",
    "plt.ylim(CAT_close_2021_03.quantile(0.5)*0.95, CAT_close_2021_03.quantile(0.5)*1.05)\n",
    "plt.legend()\n",
    "plt.grid('both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check volume data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily = df.resample('D', level=1).mean()\n",
    "df_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily['vol_div_by_99_pct'] = df_daily['volume']/df_daily['volume'].quantile(0.99)\n",
    "# df_daily.groupby(df_daily.index.weekday)['vol_div_by_99_pct'].mean()\n",
    "df_daily['vol_div_by_99_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily['vol_div_by_99_pct'].quantile([0.05,0.1,0.15,0.25, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily['vol_div_by_99_pct'][40:110].plot(kind='bar', figsize=(14,4))\n",
    "plt.title('Daily Volume divided by 99th percentile volume')\n",
    "plt.xticks(np.arange(70),df_daily['vol_div_by_99_pct'][:70].index.strftime(\"%m-%d-%y\"))\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = df_daily.assign(weekday = df_daily.index.day_name()).plot(kind='scatter', x='weekday',y='volume')\n",
    "ax.set_xticks(list(calendar.day_name))\n",
    "plt.title('Volume by day of week')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(df_daily['volume']/df_daily['volume'].quantile(0.99)).plot(kind='hist', bins=100)\n",
    "# plt.xlim(0.1,3)\n",
    "plt.title('Distribution of Daily Volume divided by 99th percentile volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily['Percent_Change'] = df_daily[['open','close']].pct_change(axis=1).iloc[:,-1]\n",
    "df_daily.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily[['Percent_Change','vol_div_by_99_pct']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily[['Percent_Change','volume']].plot(kind='scatter',\n",
    "                          x='Percent_Change', y='volume')\n",
    "plt.xlim(-0.01, 0.01)\n",
    "plt.title('Daily Open to Close Percent Change vs. Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_daily[['Percent_Change','vol_div_by_99_pct']].plot(kind='scatter',\n",
    "                          x='Percent_Change', y='vol_div_by_99_pct')\n",
    "plt.xlim(-0.01, 0.01)\n",
    "plt.title('Daily Open to Close Percent Change vs. Relative Volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Cleaning Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add Features (Technical Indicators)\n",
    "https://github.com/twopirllc/pandas-ta\n",
    "\n",
    "https://github.com/bukosabino/ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.index.levels[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "symbol = 'CAT'\n",
    "# df = df_denoised.copy()\n",
    "df = df_denoised.loc[symbol].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calculate Returns over various periods\n",
    "To capture time series dynamics that reflect, for example, momentum patterns, we compute historical returns using the method `.pct_change(n_periods)`, that is, returns over various periods as identified by lags.\n",
    "\n",
    "We then convert the wide result back to long format with the `.stack()` method, use `.pipe()` to apply the `.clip()` method to the resulting `DataFrame`, and winsorize returns at the [1%, 99%] levels; that is, we cap outliers at these percentiles.\n",
    "\n",
    "Finally, we normalize returns using the geometric average. After using `.swaplevel()` to change the order of the `MultiIndex` levels, we obtain compounded periodic returns for six periods ranging from 1 to 12 months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_return(df, n_periods, start, end):\n",
    "    \"\"\"Example:  \n",
    "    df['log_return'] = log_return(df, 1, 'open','close')\"\"\"\n",
    "    return np.log(df[end]/df[start].shift(n_periods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['log_return'] = log_return(df, 0, 'open','close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(2,1, sharex=True)\n",
    "\n",
    "df['close'].plot(ax=ax1, color = 'black')\n",
    "ax1.set_title('close')\n",
    "df['log_return'].cumsum().plot(ax=ax2, color='orange')\n",
    "ax2.set_title('Log Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# stopped Here 1-15-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_cutoff = 0.01\n",
    "data = df.copy()\n",
    "lags = [1, 2, 3, 6, 9, 12]\n",
    "for lag in lags:\n",
    "    data[f'log_return_{lag}'] = (data\n",
    "                           .pct_change(lag)\n",
    "                           .stack()\n",
    "                           .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),\n",
    "                                                  upper=x.quantile(1-outlier_cutoff)))\n",
    "                           .add(1)\n",
    "                           .pow(1/lag)\n",
    "                           .sub(1)\n",
    "                           )\n",
    "data = data.swaplevel().dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "?? ta.log_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_denoised.ta.log_return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Returns and append to the df DataFrame\n",
    "df.ta.log_return(cumulative=True, append=True)\n",
    "df.ta.percent_return(cumulative=True, append=True)\n",
    "\n",
    "# New Columns with results\n",
    "df.columns\n",
    "\n",
    "# Take a peek\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Clean NaN values\n",
    "# df = dropna(df)\n",
    "\n",
    "# Add all ta features\n",
    "df_features = add_all_ta_features(df_denoised,open=\"open\",\n",
    "          high=\"high\", low=\"low\", close=\"close\", volume=\"volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_features.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_denoised.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### De-Noise OHLC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pykalman import KalmanFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kf = KalmanFilter(transition_matrices = [1],\n",
    "                  observation_matrices = [1],\n",
    "                  initial_state_mean = 0,\n",
    "                  initial_state_covariance = 1,\n",
    "                  observation_covariance=1,\n",
    "                  transition_covariance=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_means, _ = kf.filter(sp500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Compare with moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sp500_smoothed = sp500.to_frame('close')\n",
    "sp500_smoothed['Kalman Filter'] = state_means\n",
    "for months in [1,2,3]:\n",
    "    sp500_smoothed[f'MA ({months}m)'] = sp500.rolling(window=months*21).mean()\n",
    "\n",
    "ax = sp500_smoothed.plot(title='Kalman Filter vs Moving Average', figsize=(14,6), lw=1, rot=0)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('S&P 500')\n",
    "plt.tight_layout()\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_incomplete_symbols(min_threshold_pct=20):\n",
    "    \"\"\"Return multi-index df where all symbols whose history has data for less\n",
    "    than the min_threshold_pct is omitted\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df.loc[idx[assets_query[0],:]].groupby(pd.Grouper(freq='1Min'))\n",
    "df.loc[idx[assets_query[0],:]].groupby(pd.Grouper(freq='1D')).agg({'open' : 'mean', 'high': 'mean','low': 'mean','close': 'mean','volume':'sum'})\n",
    "# .groupby(pd.Grouper(freq=freq,level=1)).agg({'open' : 'mean', 'high': 'mean','low': 'mean','close': 'mean','volume':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def price_volume(df, price='open', vol='volume', symbol=assets_query[0], freq='1Hour',fname=None):\n",
    "    # for multiindex:\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.loc[idx[symbol,:]].groupby(pd.Grouper(freq=freq)).agg({'open' : 'mean',\n",
    "                                                'high': 'mean',\n",
    "                                                'low': 'mean',\n",
    "                                                'close': 'mean',\n",
    "                                                'volume':'sum'})\n",
    "    \n",
    "#     df = df.groupby(pd.Grouper(freq=freq)).agg({'open' : 'mean',\n",
    "#                                                 'high': 'mean',\n",
    "#                                                 'low': 'mean',\n",
    "#                                                 'close': 'mean',\n",
    "#                                                 'volume':'sum'})\n",
    "#     df.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, sharex=True, figsize=(15,8))\n",
    "    axes[0].plot(df.index, df[price])\n",
    "    axes[1].bar(df.index, df[vol], width=1/(5*len(df.index)), color='r')\n",
    "    \n",
    "    # formatting\n",
    "    xfmt = mpl.dates.DateFormatter('%H:%M')\n",
    "    axes[1].xaxis.set_major_locator(mpl.dates.HourLocator(interval=3))\n",
    "#     axes[1].xaxis.set_major_formatter(xfmt)\n",
    "#     axes[1].get_xaxis().set_tick_params(which='major', pad=25)\n",
    "    axes[0].set_title(f'{price}', fontsize=14)\n",
    "    axes[1].set_title('Volume', fontsize=14)\n",
    "#     fig.autofmt_xdate()\n",
    "    fig.suptitle(symbol)\n",
    "#     fig.tight_layout()\n",
    "#     plt.subplots_adjust(top=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "price_volume(df, freq='1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[idx[assets_query[0],:]].groupby(pd.Grouper(freq='1D')).agg({'open' : 'mean', 'high': 'mean','low': 'mean','close': 'mean','volume':'sum'}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_bar_stats(agg_trades, weighted_price='close'):\n",
    "#     vwap = agg_trades.apply(lambda x: np.average(x[weighted_price], weights=x.volume)).to_frame('vwap')\n",
    "#     ohlc = agg_trades.price.ohlc()\n",
    "    vol = agg_trades.shares.sum().to_frame('vol')\n",
    "#     txn = agg_trades.shares.size().to_frame('txn')\n",
    "    return pd.concat([ohlc, vwap, vol, txn], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[idx[assets_query[0],:],].droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trades = df.loc[idx[assets_query[0],:],].droplevel(0)\n",
    "resampled = trades.groupby(pd.Grouper(freq='1H'))\n",
    "time_bars = get_bar_stats(resampled)\n",
    "# normaltest(time_bars.vwap.pct_change().dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "price_volume(time_bars, \n",
    "             suptitle=f'Time Bars | {stock} | {pd.to_datetime(date).date()}', \n",
    "             fname='time_bars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import bokeh\n",
    "bokeh.sampledata.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.sampledata.stocks import MSFT\n",
    "output_notebook()\n",
    "MSFT = pd.DataFrame(MSFT)[:50]\n",
    "#     df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "MSFT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(MSFT['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[idx[assets_query[0],:]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[idx[assets_query[0],:]].index.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[idx[assets_query[0],:]].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def candlestick_w_volume(df, symbol):\n",
    "    \"\"\"Plot OHLC Candlestick with Volume chart\"\"\"\n",
    "    from math import pi\n",
    "#     import pandas as pd\n",
    "    from bokeh.plotting import figure, output_notebook, show\n",
    "    from bokeh.layouts import gridplot\n",
    "    from bokeh.sampledata.stocks import MSFT\n",
    "    output_notebook()\n",
    "#     df = pd.DataFrame(MSFT)[:50]\n",
    "#     df[\"date\"] = pd.to_datetime(df[\"date\"]) - Original Code\n",
    "# for multiindex:\n",
    "    df = df.loc[idx[symbol,:]]\n",
    "    \n",
    "    df[\"date\"] = df.index.strftime(\"%Y-%m-%d\")\n",
    "    display(df.head(2))\n",
    "    \n",
    "\n",
    "    inc = df.close > df.open\n",
    "    dec = df.open > df.close\n",
    "    w = 12*60*60*1000 # half day in ms\n",
    "    print(inc, dec)\n",
    "\n",
    "    TOOLS = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "\n",
    "    p1 = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=700, plot_height=300, title = f\"{symbol}\")\n",
    "    p1.xaxis.visible = False\n",
    "    p1.xaxis.major_label_orientation = pi/4\n",
    "    p1.grid.grid_line_alpha=0.3\n",
    "    print(p1.x_range)\n",
    "    \n",
    "    p1.segment(df.date, df.high, df.date, df.low, color=\"black\")\n",
    "    p1.vbar(df.date[inc], w, df.open[inc], df.close[inc], fill_color=\"#D5E1DD\", line_color=\"black\")\n",
    "    p1.vbar(df.date[dec], w, df.open[dec], df.close[dec], fill_color=\"#F2583E\", line_color=\"black\")\n",
    "\n",
    "    p2 = figure(x_axis_type=\"datetime\", tools=\"\", toolbar_location=None, plot_width=700, plot_height=200, x_range=p1.x_range)\n",
    "    p2.xaxis.major_label_orientation = pi/4\n",
    "    p2.grid.grid_line_alpha=0.3\n",
    "    p2.vbar(df.date, w, df.volume, [0]*df.shape[0])\n",
    "\n",
    "    show(gridplot([[p1],[p2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "candlestick_w_volume(df, assets_query[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def candlestick_w_volume():\n",
    "    \"\"\"Plot OHLC Candlestick with Volume chart\"\"\"\n",
    "    from math import pi\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    from bokeh.plotting import figure, output_notebook, show\n",
    "    from bokeh.layouts import gridplot\n",
    "    from bokeh.sampledata.stocks import MSFT\n",
    "\n",
    "    output_notebook()\n",
    "\n",
    "    df = pd.DataFrame(MSFT)[:50]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    inc = df.close > df.open\n",
    "    dec = df.open > df.close\n",
    "    w = 12*60*60*1000 # half day in ms\n",
    "\n",
    "    TOOLS = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "\n",
    "    p1 = figure(x_axis_type=\"datetime\", tools=TOOLS, plot_width=700, plot_height=300, title = \"MSFT Candlestick with Volume\")\n",
    "    p1.xaxis.visible = False\n",
    "    p1.xaxis.major_label_orientation = pi/4\n",
    "    p1.grid.grid_line_alpha=0.3\n",
    "\n",
    "    p1.segment(df.date, df.high, df.date, df.low, color=\"black\")\n",
    "    p1.vbar(df.date[inc], w, df.open[inc], df.close[inc], fill_color=\"#D5E1DD\", line_color=\"black\")\n",
    "    p1.vbar(df.date[dec], w, df.open[dec], df.close[dec], fill_color=\"#F2583E\", line_color=\"black\")\n",
    "\n",
    "    p2 = figure(x_axis_type=\"datetime\", tools=\"\", toolbar_location=None, plot_width=700, plot_height=200, x_range=p1.x_range)\n",
    "    p2.xaxis.major_label_orientation = pi/4\n",
    "    p2.grid.grid_line_alpha=0.3\n",
    "    p2.vbar(df.date, w, df.volume, [0]*df.shape[0])\n",
    "\n",
    "    show(gridplot([[p1],[p2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "candlestick_w_volume()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save df to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multi_symbol_df_to_csv(df, directory=''):\n",
    "    start_date = datetime.strftime(df.index.get_level_values(1).min(),\"%Y-%m-%d\")\n",
    "    end_date = datetime.strftime(df.index.get_level_values(1).max(),\"%Y-%m-%d\")\n",
    "    n_symbols = df.index.get_level_values(0).nunique()\n",
    "    frequency = df.index.levels[1].freqstr\n",
    "    filename = os.path.join(directory,f'symbols_{n_symbols}_{frequency}_{start_date} to {end_date}.csv') \n",
    "    df.to_csv(filename)\n",
    "    print(f'df saved as \"{filename}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multi_symbol_df_to_csv(df, directory='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Add Technical indicators\n",
    "normalized volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import alpaca_backtrader_api\n",
    "import backtrader as bt\n",
    "# from datetime import datetime\n",
    "\n",
    "# ALPACA_API_KEY = <key_id>\n",
    "# ALPACA_SECRET_KEY = <secret_key>\n",
    "# ALPACA_PAPER = True\n",
    "\n",
    "\n",
    "class SmaCross(bt.SignalStrategy):\n",
    "  def __init__(self):\n",
    "    sma1, sma2 = bt.ind.SMA(period=10), bt.ind.SMA(period=30)\n",
    "    crossover = bt.ind.CrossOver(sma1, sma2)\n",
    "    self.signal_add(bt.SIGNAL_LONG, crossover)\n",
    "\n",
    "\n",
    "cerebro = bt.Cerebro()\n",
    "cerebro.addstrategy(SmaCross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "store = alpaca_backtrader_api.AlpacaStore(\n",
    "#     key_id=ALPACA_API_KEY,\n",
    "#     secret_key=ALPACA_SECRET_KEY,\n",
    "#     paper=ALPACA_PAPER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not ALPACA_PAPER:\n",
    "  broker = store.getbroker()  # or just alpaca_backtrader_api.AlpacaBroker()\n",
    "  cerebro.setbroker(broker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DataFactory = store.getdata  # or use alpaca_backtrader_api.AlpacaData\n",
    "data0 = DataFactory(dataname='AAPL', historical=True, fromdate=datetime(\n",
    "    2015, 1, 1), timeframe=bt.TimeFrame.Days)\n",
    "cerebro.adddata(data0)\n",
    "\n",
    "print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "cerebro.run()\n",
    "print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "cerebro.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Technical Indicator functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.677px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}